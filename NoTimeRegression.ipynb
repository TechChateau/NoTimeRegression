{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOXxEw7iF15rWeN9sJYLjbe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#@title License and Disclaimer\n","\n","'''\n","MIT License\n","\n","Copyright (c) 2023 Parham Faraji\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE.\n","'''"],"metadata":{"id":"ePBfxbwKTes_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"pKjLoTZ7UQKQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703886707899,"user_tz":-210,"elapsed":4750,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}},"outputId":"2783c5a5-95fb-4db5-b93c-a1e1cf72f8e8"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You use TensorFlow DType <dtype: 'float32'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to float32.\n","WARNING:absl:You use TensorFlow DType <dtype: 'float64'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to float64.\n"]}],"source":["#@title Data Import\n","\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","# Construct a tf.data.Dataset\n","train = tfds.load('wine_quality/white', split='train', shuffle_files=False)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"tKSGDdUIelu4","executionInfo":{"status":"ok","timestamp":1703886707900,"user_tz":-210,"elapsed":10,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}}},"outputs":[],"source":["#@title Packaging Function\n","\n","import numpy as np\n","import cv2\n","from tqdm import tqdm\n","\n","def normalize(cluster):\n","  clusterNorm = np.zeros_like(cluster)\n","\n","  for f in range(cluster.shape[1]):\n","    offset = cluster[:,f] - np.min(cluster[:,f])\n","    clusterNorm[:,f] = offset / np.max(offset)\n","    del offset\n","\n","  print(' min:', np.min(clusterNorm, axis=0), '\\n max:',\n","                 np.max(clusterNorm, axis=0), '\\n')\n","  del cluster\n","  return np.reshape(clusterNorm, [clusterNorm.shape[0],\n","                                  clusterNorm.shape[1], 1])\n","\n","def package(data, count):\n","  # dataset = dataset.take(3)\n","  # list(dataset.as_numpy_iterator())\n","  given = []\n","  regress = []\n","  for sample in tqdm(data.take(count)):\n","\n","    features = sample['features']\n","\n","    given.append([\n","    features['alcohol'],\n","    features['chlorides'],\n","    features['citric acid'],\n","    features['density'],\n","    features['free sulfur dioxide'],\n","    features['pH'],\n","    features['residual sugar'],\n","    features['sulphates'],\n","    features['total sulfur dioxide'],\n","    features['volatile acidity']\n","    ])\n","\n","    regress.append([\n","    sample['quality'],\n","    features['fixed acidity']\n","    ])\n","\n","  given_out = np.array(given)\n","  regress_out = np.array(regress)\n","  del given, regress\n","  print(given_out.shape, regress_out.shape, '\\n')\n","  return given_out, regress_out\n"]},{"cell_type":"code","source":["#@title Packaging Data\n","# del V, Yv, X, Yx\n","giv, reg = package(train, 4898) #4898\n","giv = normalize(giv)\n","reg = normalize(reg)\n","\n","split = 898\n","X, Yx, V, Yv= giv[split:], reg[split:], giv[:split], reg[:split]\n","\n","Yv = np.reshape(Yv, [Yv.shape[0], Yv.shape[1]])\n","Yx = np.reshape(Yx, [Yx.shape[0], Yx.shape[1]])\n","\n","print(X.shape, V.shape)\n","print(Yx.shape, Yv.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUHvhAL77kRe","executionInfo":{"status":"ok","timestamp":1703886710936,"user_tz":-210,"elapsed":3044,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}},"outputId":"9e0b6d14-a9e8-4b63-a294-0bb5deef803f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4898/4898 [00:02<00:00, 1894.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(4898, 10) (4898, 2) \n","\n"," min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n"," max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] \n","\n"," min: [0. 0.] \n"," max: [1. 1.] \n","\n","(4000, 10, 1) (898, 10, 1)\n","(4000, 2) (898, 2)\n"]}]},{"cell_type":"code","source":["#@title Layer Designer (1D) Function\n","\n","from tensorflow import keras\n","\n","# This approach helps make design intuitive, intelligible, and easy to alter\n","# No need to worry about the order of operations\n","# You simply enable certain sub-layers and focus on the grand architecture\n","# It is especially useful as a tool for beginners to start from the middle by fiddling around\n","# The \"engage\" variable in the function is responsible for enabling all operations needed for a composite layer\n","# Using default parameters, layers can be on autopilot by verifying input/\"consign\" alone\n","def layer(consign, engage=[1,0,0,1,0,0], fmaps=24, k=3, act='relu', step=1, pool=2, scale=2, dp=.20, ep=1e-5, pad='same'): # same/valid\n","\n","  # preset-choices:\n","  if engage[0]==1:\n","    consign = keras.layers.Conv1D(filters=fmaps, kernel_size=k, strides=step, padding=pad)(consign)\n","  if engage[0]==2:\n","    consign = keras.layers.Conv1DTranspose(filters=fmaps, kernel_size=k, strides=step, padding=pad)(consign)\n","  if engage[0]==3:\n","    attn = keras.layers.MultiHeadAttention(num_heads=3, key_dim=fmaps)\n","    consign = attn(consign, consign)\n","\n","  if engage[1]:\n","    consign = keras.layers.BatchNormalization()(consign)\n","  if engage[2]:\n","    consign = keras.layers.LayerNormalization(epsilon=ep)(consign)\n","  if engage[3]:\n","    consign = keras.layers.Activation(act)(consign)\n","  if engage[4]:\n","    consign = keras.layers.MaxPooling1D(pool)(consign)\n","  if engage[5]:\n","    consign = keras.layers.UpSampling1D(scale)(consign)\n","  if engage[6]:\n","    consign = keras.layers.Dropout(dp)(consign)\n","\n","  return consign\n"],"metadata":{"id":"ah2k-PVTUxau","executionInfo":{"status":"ok","timestamp":1703886710936,"user_tz":-210,"elapsed":5,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#@title 1D Regression Model Demo\n","\n","'''1-dimensionals are cost efficient, so here we went with a depth of 7-8 layers, and width of 3-4, with as much demonstrated diversity'''\n","def architect(nc=2): # nc = number of output values\n","\n","  # lunch\n","  inputshape = [10,1]\n","  inputlayer = keras.layers.Input(inputshape)\n","\n","  # block a @lvl1\n","  a = layer(inputlayer, [1, 1,0, 1, 0,0,0], fmaps=30, k=3, act='relu')\n","  a = layer(a,          [1, 1,0, 1, 0,0,0], fmaps=30, k=3, act='relu')\n","  a = layer(a,          [1, 1,0, 1, 0,0,0], fmaps=30, k=2, act='relu')\n","\n","  # block b @lvl1\n","  b = layer(inputlayer, [0, 0,0, 0, 0,1,0], scale=2)\n","  b = layer(b,          [1, 1,0, 1, 0,0,0], fmaps=30, k=5, act='relu', pad='valid')\n","  b = layer(b,          [1, 1,0, 1, 0,0,1], fmaps=30, k=4, act='relu', pad='valid')\n","  b = layer(b,          [1, 1,0, 1, 0,0,0], fmaps=30, k=4, act='relu', pad='valid')\n","\n","  # block c @lvl1\n","  c = layer(inputlayer, [2, 0,0, 1, 0,0,0], fmaps=30, k=4, step=2, act='relu')\n","  c = layer(c,          [1, 0,0, 1, 0,0,0], fmaps=30, k=4, act='relu')\n","  c = layer(c,          [1, 0,0, 1, 1,0,0], fmaps=30, k=4, pool=2, act='relu')\n","\n","  # res=inputlayer\n","\n","  # merge\n","  con = keras.layers.Concatenate()([inputlayer, a, b, c])\n","  con = keras.layers.BatchNormalization()(con)\n","\n","  # attention bypass\n","  a2 = layer(b,     [3, 0,1, 0, 0,0,0], fmaps=30)\n","  a2 = layer(a2,    [1, 1,0, 1, 0,0,1], fmaps=20, k=3, act='relu')\n","\n","  # 2nd lvl independent layers\n","  b2 = layer(con,   [1, 0,0, 1, 0,0,0], fmaps=40, k=5, act='relu')\n","\n","  c2 = layer(con,   [1, 1,0, 1, 0,0,0], fmaps=40, k=4, act='relu')\n","\n","  d2 = layer(con,   [2, 1,0, 1, 0,0,0], fmaps=40, k=4, act='relu')\n","\n","  # res 2\n","  res2 = layer(con, [1, 1,0, 1, 0,0,0], fmaps=30, k=1, act='linear')\n","\n","  # merge 2\n","  con2 = keras.layers.Concatenate()([res2, a2, b2, c2, d2])\n","  con2 = keras.layers.BatchNormalization()(con2)\n","\n","  # 3rd lvl independent layers\n","  a3 = layer(con2, [2, 1,0, 1, 0,0,0], fmaps=40, k=4, act='relu')\n","\n","  b3 = layer(con2, [1, 1,0, 1, 0,0,0], fmaps=40, k=5, act='relu')\n","\n","  c3 = layer(con2, [1, 1,0, 1, 0,0,0], fmaps=40, k=3, act='relu')\n","\n","  d3 = layer(b3,   [1, 0,0, 1, 0,0,1], fmaps=40, k=4, act='relu')\n","\n","  # res 3\n","  res3 = layer(con2, [1, 1,0, 1, 0,0,0], fmaps=30, k=1, act='linear')\n","\n","  # merge 3\n","  con3 = keras.layers.Concatenate()([res3, a3, b3, c3, d3])\n","  con3 = keras.layers.BatchNormalization()(con3)\n","\n","  # finalization\n","  f = layer(con3, [1, 1,0, 1, 0,0,0], fmaps=50, k=3, act='relu')\n","  f = layer(f,    [1, 1,0, 1, 0,0,0], fmaps=60, k=3, act='relu')\n","\n","  # res 4\n","  res4 = layer(con3, [1, 1,0, 1, 0,0,0], fmaps=40, k=1, act='linear')\n","\n","  # merge 4\n","  con4 = keras.layers.Concatenate()([res4, f])\n","  # con4 = keras.layers.BatchNormalization()(con4)\n","\n","  gap = keras.layers.GlobalAveragePooling1D()(con4)\n","  outputlayer = keras.layers.Dense(nc, activation='sigmoid')(gap)\n","\n","  return inputlayer, outputlayer\n","\n","\n","# tie regression model\n","input, output = architect()\n","model = keras.models.Model(inputs=input, outputs=output)\n","\n","# compile model\n","model.compile(loss='mae', optimizer=keras.optimizers.Adam())\n","\n","# print model\n","model.summary()\n","\n","# implement callbacks\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8,\n","                patience=5, min_lr=1e-4)\n","\n","es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto',\n","                verbose=1, patience=35, restore_best_weights=True)\n","\n","cp = keras.callbacks.ModelCheckpoint(\n","                './best_checkpoint', monitor=\"val_loss\", verbose=0,\n","                save_best_only=True, save_weights_only=False, mode=\"auto\",\n","                save_freq=\"epoch\", initial_value_threshold=0.0487) # *threshold based on a previous test run\n","\n","# fit data into model\n","model.fit(x=X, y=Yx, batch_size=308, epochs=270, verbose=1, # *batch size based on available hardware\n","                validation_data=(V, Yv), shuffle=True,\n","                callbacks=[reduce_lr, es, cp]) # or [reduce_lr, es]\n","\n","# Save model\n","model.save('NTR')"],"metadata":{"id":"cBvRNmChlL3j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703887826645,"user_tz":-210,"elapsed":224157,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}},"outputId":"3302cf6c-b5f2-4fb9-ff7d-a958c0fe19a8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_5 (InputLayer)        [(None, 10, 1)]              0         []                            \n","                                                                                                  \n"," up_sampling1d_4 (UpSamplin  (None, 20, 1)                0         ['input_5[0][0]']             \n"," g1D)                                                                                             \n","                                                                                                  \n"," conv1d_79 (Conv1D)          (None, 16, 30)               180       ['up_sampling1d_4[0][0]']     \n","                                                                                                  \n"," batch_normalization_86 (Ba  (None, 16, 30)               120       ['conv1d_79[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," conv1d_76 (Conv1D)          (None, 10, 30)               120       ['input_5[0][0]']             \n","                                                                                                  \n"," activation_91 (Activation)  (None, 16, 30)               0         ['batch_normalization_86[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," batch_normalization_83 (Ba  (None, 10, 30)               120       ['conv1d_76[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," conv1d_80 (Conv1D)          (None, 13, 30)               3630      ['activation_91[0][0]']       \n","                                                                                                  \n"," activation_88 (Activation)  (None, 10, 30)               0         ['batch_normalization_83[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," batch_normalization_87 (Ba  (None, 13, 30)               120       ['conv1d_80[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," conv1d_transpose_12 (Conv1  (None, 20, 30)               150       ['input_5[0][0]']             \n"," DTranspose)                                                                                      \n","                                                                                                  \n"," conv1d_77 (Conv1D)          (None, 10, 30)               2730      ['activation_88[0][0]']       \n","                                                                                                  \n"," activation_92 (Activation)  (None, 13, 30)               0         ['batch_normalization_87[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," activation_94 (Activation)  (None, 20, 30)               0         ['conv1d_transpose_12[0][0]'] \n","                                                                                                  \n"," batch_normalization_84 (Ba  (None, 10, 30)               120       ['conv1d_77[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," dropout_12 (Dropout)        (None, 13, 30)               0         ['activation_92[0][0]']       \n","                                                                                                  \n"," conv1d_82 (Conv1D)          (None, 20, 30)               3630      ['activation_94[0][0]']       \n","                                                                                                  \n"," activation_89 (Activation)  (None, 10, 30)               0         ['batch_normalization_84[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv1d_81 (Conv1D)          (None, 10, 30)               3630      ['dropout_12[0][0]']          \n","                                                                                                  \n"," activation_95 (Activation)  (None, 20, 30)               0         ['conv1d_82[0][0]']           \n","                                                                                                  \n"," conv1d_78 (Conv1D)          (None, 10, 30)               1830      ['activation_89[0][0]']       \n","                                                                                                  \n"," batch_normalization_88 (Ba  (None, 10, 30)               120       ['conv1d_81[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," conv1d_83 (Conv1D)          (None, 20, 30)               3630      ['activation_95[0][0]']       \n","                                                                                                  \n"," batch_normalization_85 (Ba  (None, 10, 30)               120       ['conv1d_78[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_93 (Activation)  (None, 10, 30)               0         ['batch_normalization_88[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," activation_96 (Activation)  (None, 20, 30)               0         ['conv1d_83[0][0]']           \n","                                                                                                  \n"," activation_90 (Activation)  (None, 10, 30)               0         ['batch_normalization_85[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," max_pooling1d_4 (MaxPoolin  (None, 10, 30)               0         ['activation_96[0][0]']       \n"," g1D)                                                                                             \n","                                                                                                  \n"," multi_head_attention_4 (Mu  (None, 10, 30)               11100     ['activation_93[0][0]',       \n"," ltiHeadAttention)                                                   'activation_93[0][0]']       \n","                                                                                                  \n"," concatenate_16 (Concatenat  (None, 10, 91)               0         ['input_5[0][0]',             \n"," e)                                                                  'activation_90[0][0]',       \n","                                                                     'activation_93[0][0]',       \n","                                                                     'max_pooling1d_4[0][0]']     \n","                                                                                                  \n"," layer_normalization_4 (Lay  (None, 10, 30)               60        ['multi_head_attention_4[0][0]\n"," erNormalization)                                                   ']                            \n","                                                                                                  \n"," batch_normalization_89 (Ba  (None, 10, 91)               364       ['concatenate_16[0][0]']      \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," conv1d_84 (Conv1D)          (None, 10, 20)               1820      ['layer_normalization_4[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," conv1d_87 (Conv1D)          (None, 10, 30)               2760      ['batch_normalization_89[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," batch_normalization_90 (Ba  (None, 10, 20)               80        ['conv1d_84[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," conv1d_86 (Conv1D)          (None, 10, 40)               14600     ['batch_normalization_89[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv1d_transpose_13 (Conv1  (None, 10, 40)               14600     ['batch_normalization_89[0][0]\n"," DTranspose)                                                        ']                            \n","                                                                                                  \n"," batch_normalization_93 (Ba  (None, 10, 30)               120       ['conv1d_87[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_97 (Activation)  (None, 10, 20)               0         ['batch_normalization_90[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv1d_85 (Conv1D)          (None, 10, 40)               18240     ['batch_normalization_89[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," batch_normalization_91 (Ba  (None, 10, 40)               160       ['conv1d_86[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," batch_normalization_92 (Ba  (None, 10, 40)               160       ['conv1d_transpose_13[0][0]'] \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_101 (Activation  (None, 10, 30)               0         ['batch_normalization_93[0][0]\n"," )                                                                  ']                            \n","                                                                                                  \n"," dropout_13 (Dropout)        (None, 10, 20)               0         ['activation_97[0][0]']       \n","                                                                                                  \n"," activation_98 (Activation)  (None, 10, 40)               0         ['conv1d_85[0][0]']           \n","                                                                                                  \n"," activation_99 (Activation)  (None, 10, 40)               0         ['batch_normalization_91[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," activation_100 (Activation  (None, 10, 40)               0         ['batch_normalization_92[0][0]\n"," )                                                                  ']                            \n","                                                                                                  \n"," concatenate_17 (Concatenat  (None, 10, 170)              0         ['activation_101[0][0]',      \n"," e)                                                                  'dropout_13[0][0]',          \n","                                                                     'activation_98[0][0]',       \n","                                                                     'activation_99[0][0]',       \n","                                                                     'activation_100[0][0]']      \n","                                                                                                  \n"," batch_normalization_94 (Ba  (None, 10, 170)              680       ['concatenate_17[0][0]']      \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," conv1d_88 (Conv1D)          (None, 10, 40)               34040     ['batch_normalization_94[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," batch_normalization_96 (Ba  (None, 10, 40)               160       ['conv1d_88[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_103 (Activation  (None, 10, 40)               0         ['batch_normalization_96[0][0]\n"," )                                                                  ']                            \n","                                                                                                  \n"," conv1d_91 (Conv1D)          (None, 10, 30)               5130      ['batch_normalization_94[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv1d_transpose_14 (Conv1  (None, 10, 40)               27240     ['batch_normalization_94[0][0]\n"," DTranspose)                                                        ']                            \n","                                                                                                  \n"," conv1d_89 (Conv1D)          (None, 10, 40)               20440     ['batch_normalization_94[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv1d_90 (Conv1D)          (None, 10, 40)               6440      ['activation_103[0][0]']      \n","                                                                                                  \n"," batch_normalization_98 (Ba  (None, 10, 30)               120       ['conv1d_91[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," batch_normalization_95 (Ba  (None, 10, 40)               160       ['conv1d_transpose_14[0][0]'] \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," batch_normalization_97 (Ba  (None, 10, 40)               160       ['conv1d_89[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_105 (Activation  (None, 10, 40)               0         ['conv1d_90[0][0]']           \n"," )                                                                                                \n","                                                                                                  \n"," activation_106 (Activation  (None, 10, 30)               0         ['batch_normalization_98[0][0]\n"," )                                                                  ']                            \n","                                                                                                  \n"," activation_102 (Activation  (None, 10, 40)               0         ['batch_normalization_95[0][0]\n"," )                                                                  ']                            \n","                                                                                                  \n"," activation_104 (Activation  (None, 10, 40)               0         ['batch_normalization_97[0][0]\n"," )                                                                  ']                            \n","                                                                                                  \n"," dropout_14 (Dropout)        (None, 10, 40)               0         ['activation_105[0][0]']      \n","                                                                                                  \n"," concatenate_18 (Concatenat  (None, 10, 190)              0         ['activation_106[0][0]',      \n"," e)                                                                  'activation_102[0][0]',      \n","                                                                     'activation_103[0][0]',      \n","                                                                     'activation_104[0][0]',      \n","                                                                     'dropout_14[0][0]']          \n","                                                                                                  \n"," batch_normalization_99 (Ba  (None, 10, 190)              760       ['concatenate_18[0][0]']      \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," conv1d_92 (Conv1D)          (None, 10, 50)               28550     ['batch_normalization_99[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," batch_normalization_100 (B  (None, 10, 50)               200       ['conv1d_92[0][0]']           \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," activation_107 (Activation  (None, 10, 50)               0         ['batch_normalization_100[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," conv1d_94 (Conv1D)          (None, 10, 40)               7640      ['batch_normalization_99[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv1d_93 (Conv1D)          (None, 10, 60)               9060      ['activation_107[0][0]']      \n","                                                                                                  \n"," batch_normalization_102 (B  (None, 10, 40)               160       ['conv1d_94[0][0]']           \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," batch_normalization_101 (B  (None, 10, 60)               240       ['conv1d_93[0][0]']           \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," activation_109 (Activation  (None, 10, 40)               0         ['batch_normalization_102[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," activation_108 (Activation  (None, 10, 60)               0         ['batch_normalization_101[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," concatenate_19 (Concatenat  (None, 10, 100)              0         ['activation_109[0][0]',      \n"," e)                                                                  'activation_108[0][0]']      \n","                                                                                                  \n"," global_average_pooling1d_4  (None, 100)                  0         ['concatenate_19[0][0]']      \n","  (GlobalAveragePooling1D)                                                                        \n","                                                                                                  \n"," dense_4 (Dense)             (None, 2)                    202       ['global_average_pooling1d_4[0\n","                                                                    ][0]']                        \n","                                                                                                  \n","==================================================================================================\n","Total params: 225696 (881.62 KB)\n","Trainable params: 223574 (873.34 KB)\n","Non-trainable params: 2122 (8.29 KB)\n","__________________________________________________________________________________________________\n","Epoch 1/270\n","13/13 [==============================] - 17s 146ms/step - loss: 0.0970 - val_loss: 0.1357 - lr: 0.0010\n","Epoch 2/270\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0733 - val_loss: 0.1392 - lr: 0.0010\n","Epoch 3/270\n","13/13 [==============================] - 1s 54ms/step - loss: 0.0692 - val_loss: 0.1400 - lr: 0.0010\n","Epoch 4/270\n","13/13 [==============================] - 1s 42ms/step - loss: 0.0665 - val_loss: 0.1387 - lr: 0.0010\n","Epoch 5/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0637 - val_loss: 0.1387 - lr: 0.0010\n","Epoch 6/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0618 - val_loss: 0.1451 - lr: 0.0010\n","Epoch 7/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0619 - val_loss: 0.1389 - lr: 0.0010\n","Epoch 8/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0614 - val_loss: 0.1435 - lr: 0.0010\n","Epoch 9/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0596 - val_loss: 0.1410 - lr: 0.0010\n","Epoch 10/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0582 - val_loss: 0.1325 - lr: 0.0010\n","Epoch 11/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0577 - val_loss: 0.1421 - lr: 0.0010\n","Epoch 12/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0580 - val_loss: 0.1379 - lr: 0.0010\n","Epoch 13/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0572 - val_loss: 0.1264 - lr: 0.0010\n","Epoch 14/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0557 - val_loss: 0.1217 - lr: 0.0010\n","Epoch 15/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0557 - val_loss: 0.1264 - lr: 0.0010\n","Epoch 16/270\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0548 - val_loss: 0.1189 - lr: 0.0010\n","Epoch 17/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0540 - val_loss: 0.1067 - lr: 0.0010\n","Epoch 18/270\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0532 - val_loss: 0.1088 - lr: 0.0010\n","Epoch 19/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0529 - val_loss: 0.1199 - lr: 0.0010\n","Epoch 20/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0520 - val_loss: 0.1191 - lr: 0.0010\n","Epoch 21/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0516 - val_loss: 0.1150 - lr: 0.0010\n","Epoch 22/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0510 - val_loss: 0.1202 - lr: 0.0010\n","Epoch 23/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0500 - val_loss: 0.1194 - lr: 0.0010\n","Epoch 24/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0508 - val_loss: 0.1111 - lr: 0.0010\n","Epoch 25/270\n","13/13 [==============================] - 1s 59ms/step - loss: 0.0498 - val_loss: 0.1100 - lr: 0.0010\n","Epoch 26/270\n","13/13 [==============================] - 1s 55ms/step - loss: 0.0500 - val_loss: 0.1136 - lr: 0.0010\n","Epoch 27/270\n","13/13 [==============================] - 1s 54ms/step - loss: 0.0493 - val_loss: 0.1113 - lr: 0.0010\n","Epoch 28/270\n","13/13 [==============================] - 1s 55ms/step - loss: 0.0484 - val_loss: 0.1078 - lr: 0.0010\n","Epoch 29/270\n","13/13 [==============================] - 1s 37ms/step - loss: 0.0476 - val_loss: 0.1029 - lr: 0.0010\n","Epoch 30/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0464 - val_loss: 0.1128 - lr: 0.0010\n","Epoch 31/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0461 - val_loss: 0.1062 - lr: 0.0010\n","Epoch 32/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0455 - val_loss: 0.1047 - lr: 0.0010\n","Epoch 33/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0453 - val_loss: 0.1104 - lr: 0.0010\n","Epoch 34/270\n","13/13 [==============================] - 0s 33ms/step - loss: 0.0453 - val_loss: 0.1033 - lr: 0.0010\n","Epoch 35/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0443 - val_loss: 0.1081 - lr: 0.0010\n","Epoch 36/270\n","13/13 [==============================] - 0s 39ms/step - loss: 0.0441 - val_loss: 0.0957 - lr: 0.0010\n","Epoch 37/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0445 - val_loss: 0.0987 - lr: 0.0010\n","Epoch 38/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0443 - val_loss: 0.0977 - lr: 0.0010\n","Epoch 39/270\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0428 - val_loss: 0.0905 - lr: 0.0010\n","Epoch 40/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0423 - val_loss: 0.0885 - lr: 0.0010\n","Epoch 41/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0421 - val_loss: 0.0907 - lr: 0.0010\n","Epoch 42/270\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0415 - val_loss: 0.0880 - lr: 0.0010\n","Epoch 43/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0413 - val_loss: 0.0888 - lr: 0.0010\n","Epoch 44/270\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0405 - val_loss: 0.0868 - lr: 0.0010\n","Epoch 45/270\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0411 - val_loss: 0.0804 - lr: 0.0010\n","Epoch 46/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0406 - val_loss: 0.0796 - lr: 0.0010\n","Epoch 47/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0397 - val_loss: 0.0770 - lr: 0.0010\n","Epoch 48/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0400 - val_loss: 0.0729 - lr: 0.0010\n","Epoch 49/270\n","13/13 [==============================] - 1s 50ms/step - loss: 0.0391 - val_loss: 0.0766 - lr: 0.0010\n","Epoch 50/270\n","13/13 [==============================] - 1s 50ms/step - loss: 0.0384 - val_loss: 0.0753 - lr: 0.0010\n","Epoch 51/270\n","13/13 [==============================] - 1s 55ms/step - loss: 0.0382 - val_loss: 0.0699 - lr: 0.0010\n","Epoch 52/270\n","13/13 [==============================] - 1s 51ms/step - loss: 0.0386 - val_loss: 0.0643 - lr: 0.0010\n","Epoch 53/270\n","13/13 [==============================] - 1s 51ms/step - loss: 0.0384 - val_loss: 0.0650 - lr: 0.0010\n","Epoch 54/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0379 - val_loss: 0.0644 - lr: 0.0010\n","Epoch 55/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0374 - val_loss: 0.0641 - lr: 0.0010\n","Epoch 56/270\n","13/13 [==============================] - 1s 42ms/step - loss: 0.0371 - val_loss: 0.0598 - lr: 0.0010\n","Epoch 57/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0354 - val_loss: 0.0589 - lr: 0.0010\n","Epoch 58/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0357 - val_loss: 0.0609 - lr: 0.0010\n","Epoch 59/270\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0351 - val_loss: 0.0592 - lr: 0.0010\n","Epoch 60/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0352 - val_loss: 0.0582 - lr: 0.0010\n","Epoch 61/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0354 - val_loss: 0.0580 - lr: 0.0010\n","Epoch 62/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0344 - val_loss: 0.0591 - lr: 0.0010\n","Epoch 63/270\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0338 - val_loss: 0.0582 - lr: 0.0010\n","Epoch 64/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0327 - val_loss: 0.0646 - lr: 0.0010\n","Epoch 65/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0335 - val_loss: 0.0581 - lr: 0.0010\n","Epoch 66/270\n","13/13 [==============================] - 0s 39ms/step - loss: 0.0331 - val_loss: 0.0576 - lr: 0.0010\n","Epoch 67/270\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0326 - val_loss: 0.0568 - lr: 0.0010\n","Epoch 68/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0323 - val_loss: 0.0575 - lr: 0.0010\n","Epoch 69/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0321 - val_loss: 0.0585 - lr: 0.0010\n","Epoch 70/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0315 - val_loss: 0.0573 - lr: 0.0010\n","Epoch 71/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0324 - val_loss: 0.0607 - lr: 0.0010\n","Epoch 72/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0317 - val_loss: 0.0596 - lr: 0.0010\n","Epoch 73/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0311 - val_loss: 0.0565 - lr: 0.0010\n","Epoch 74/270\n","13/13 [==============================] - 1s 49ms/step - loss: 0.0313 - val_loss: 0.0585 - lr: 0.0010\n","Epoch 75/270\n","13/13 [==============================] - 1s 55ms/step - loss: 0.0310 - val_loss: 0.0578 - lr: 0.0010\n","Epoch 76/270\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0315 - val_loss: 0.0561 - lr: 0.0010\n","Epoch 77/270\n","13/13 [==============================] - 1s 49ms/step - loss: 0.0306 - val_loss: 0.0565 - lr: 0.0010\n","Epoch 78/270\n","13/13 [==============================] - 1s 46ms/step - loss: 0.0296 - val_loss: 0.0564 - lr: 0.0010\n","Epoch 79/270\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0293 - val_loss: 0.0577 - lr: 0.0010\n","Epoch 80/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0287 - val_loss: 0.0569 - lr: 0.0010\n","Epoch 81/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0289 - val_loss: 0.0582 - lr: 0.0010\n","Epoch 82/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0281 - val_loss: 0.0574 - lr: 0.0010\n","Epoch 83/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0273 - val_loss: 0.0548 - lr: 0.0010\n","Epoch 84/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0277 - val_loss: 0.0569 - lr: 0.0010\n","Epoch 85/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0275 - val_loss: 0.0568 - lr: 0.0010\n","Epoch 86/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0278 - val_loss: 0.0547 - lr: 0.0010\n","Epoch 87/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0273 - val_loss: 0.0563 - lr: 0.0010\n","Epoch 88/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0283 - val_loss: 0.0569 - lr: 0.0010\n","Epoch 89/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0266 - val_loss: 0.0551 - lr: 8.0000e-04\n","Epoch 90/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0255 - val_loss: 0.0544 - lr: 8.0000e-04\n","Epoch 91/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0260 - val_loss: 0.0556 - lr: 8.0000e-04\n","Epoch 92/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0263 - val_loss: 0.0540 - lr: 8.0000e-04\n","Epoch 93/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0249 - val_loss: 0.0570 - lr: 8.0000e-04\n","Epoch 94/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0243 - val_loss: 0.0565 - lr: 8.0000e-04\n","Epoch 95/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0241 - val_loss: 0.0596 - lr: 8.0000e-04\n","Epoch 96/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0235 - val_loss: 0.0569 - lr: 8.0000e-04\n","Epoch 97/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0241 - val_loss: 0.0570 - lr: 8.0000e-04\n","Epoch 98/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0236 - val_loss: 0.0549 - lr: 8.0000e-04\n","Epoch 99/270\n","13/13 [==============================] - 1s 52ms/step - loss: 0.0241 - val_loss: 0.0561 - lr: 8.0000e-04\n","Epoch 100/270\n","13/13 [==============================] - 1s 51ms/step - loss: 0.0240 - val_loss: 0.0573 - lr: 8.0000e-04\n","Epoch 101/270\n","13/13 [==============================] - 1s 51ms/step - loss: 0.0248 - val_loss: 0.0545 - lr: 8.0000e-04\n","Epoch 102/270\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0230 - val_loss: 0.0552 - lr: 6.4000e-04\n","Epoch 103/270\n","13/13 [==============================] - 1s 52ms/step - loss: 0.0222 - val_loss: 0.0553 - lr: 6.4000e-04\n","Epoch 104/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0217 - val_loss: 0.0523 - lr: 6.4000e-04\n","Epoch 105/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0215 - val_loss: 0.0548 - lr: 6.4000e-04\n","Epoch 106/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0223 - val_loss: 0.0522 - lr: 6.4000e-04\n","Epoch 107/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0209 - val_loss: 0.0522 - lr: 6.4000e-04\n","Epoch 108/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0214 - val_loss: 0.0532 - lr: 6.4000e-04\n","Epoch 109/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0219 - val_loss: 0.0547 - lr: 6.4000e-04\n","Epoch 110/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0211 - val_loss: 0.0524 - lr: 6.4000e-04\n","Epoch 111/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0208 - val_loss: 0.0556 - lr: 6.4000e-04\n","Epoch 112/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0205 - val_loss: 0.0536 - lr: 6.4000e-04\n","Epoch 113/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0200 - val_loss: 0.0545 - lr: 6.4000e-04\n","Epoch 114/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0203 - val_loss: 0.0534 - lr: 6.4000e-04\n","Epoch 115/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0202 - val_loss: 0.0550 - lr: 6.4000e-04\n","Epoch 116/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0196 - val_loss: 0.0533 - lr: 6.4000e-04\n","Epoch 117/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0198 - val_loss: 0.0530 - lr: 6.4000e-04\n","Epoch 118/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0201 - val_loss: 0.0538 - lr: 6.4000e-04\n","Epoch 119/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0205 - val_loss: 0.0527 - lr: 6.4000e-04\n","Epoch 120/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0193 - val_loss: 0.0524 - lr: 6.4000e-04\n","Epoch 121/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0196 - val_loss: 0.0529 - lr: 6.4000e-04\n","Epoch 122/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0196 - val_loss: 0.0548 - lr: 6.4000e-04\n","Epoch 123/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0188 - val_loss: 0.0538 - lr: 6.4000e-04\n","Epoch 124/270\n","13/13 [==============================] - 1s 45ms/step - loss: 0.0194 - val_loss: 0.0552 - lr: 6.4000e-04\n","Epoch 125/270\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0197 - val_loss: 0.0545 - lr: 6.4000e-04\n","Epoch 126/270\n","13/13 [==============================] - 1s 51ms/step - loss: 0.0199 - val_loss: 0.0550 - lr: 6.4000e-04\n","Epoch 127/270\n","13/13 [==============================] - 1s 51ms/step - loss: 0.0192 - val_loss: 0.0543 - lr: 6.4000e-04\n","Epoch 128/270\n","13/13 [==============================] - 1s 52ms/step - loss: 0.0206 - val_loss: 0.0530 - lr: 6.4000e-04\n","Epoch 129/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0181 - val_loss: 0.0517 - lr: 5.1200e-04\n","Epoch 130/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0176 - val_loss: 0.0524 - lr: 5.1200e-04\n","Epoch 131/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0173 - val_loss: 0.0520 - lr: 5.1200e-04\n","Epoch 132/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0170 - val_loss: 0.0520 - lr: 5.1200e-04\n","Epoch 133/270\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0173 - val_loss: 0.0516 - lr: 5.1200e-04\n","Epoch 134/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0176 - val_loss: 0.0527 - lr: 5.1200e-04\n","Epoch 135/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0173 - val_loss: 0.0521 - lr: 5.1200e-04\n","Epoch 136/270\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0174 - val_loss: 0.0522 - lr: 5.1200e-04\n","Epoch 137/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0180 - val_loss: 0.0524 - lr: 5.1200e-04\n","Epoch 138/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0172 - val_loss: 0.0516 - lr: 4.0960e-04\n","Epoch 139/270\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0165 - val_loss: 0.0516 - lr: 4.0960e-04\n","Epoch 140/270\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0159 - val_loss: 0.0513 - lr: 4.0960e-04\n","Epoch 141/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0158 - val_loss: 0.0506 - lr: 4.0960e-04\n","Epoch 142/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0161 - val_loss: 0.0515 - lr: 4.0960e-04\n","Epoch 143/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0158 - val_loss: 0.0513 - lr: 4.0960e-04\n","Epoch 144/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0164 - val_loss: 0.0522 - lr: 4.0960e-04\n","Epoch 145/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0161 - val_loss: 0.0511 - lr: 4.0960e-04\n","Epoch 146/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0153 - val_loss: 0.0509 - lr: 3.2768e-04\n","Epoch 147/270\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0148 - val_loss: 0.0501 - lr: 3.2768e-04\n","Epoch 148/270\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0145 - val_loss: 0.0500 - lr: 3.2768e-04\n","Epoch 149/270\n","13/13 [==============================] - 1s 51ms/step - loss: 0.0152 - val_loss: 0.0506 - lr: 3.2768e-04\n","Epoch 150/270\n","13/13 [==============================] - 1s 51ms/step - loss: 0.0152 - val_loss: 0.0513 - lr: 3.2768e-04\n","Epoch 151/270\n","13/13 [==============================] - 1s 49ms/step - loss: 0.0151 - val_loss: 0.0511 - lr: 3.2768e-04\n","Epoch 152/270\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0152 - val_loss: 0.0500 - lr: 3.2768e-04\n","Epoch 153/270\n","13/13 [==============================] - 1s 47ms/step - loss: 0.0146 - val_loss: 0.0509 - lr: 3.2768e-04\n","Epoch 154/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0147 - val_loss: 0.0512 - lr: 2.6214e-04\n","Epoch 155/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0138 - val_loss: 0.0502 - lr: 2.6214e-04\n","Epoch 156/270\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0145 - val_loss: 0.0499 - lr: 2.6214e-04\n","Epoch 157/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0139 - val_loss: 0.0500 - lr: 2.6214e-04\n","Epoch 158/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0134 - val_loss: 0.0502 - lr: 2.6214e-04\n","Epoch 159/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0134 - val_loss: 0.0501 - lr: 2.6214e-04\n","Epoch 160/270\n","13/13 [==============================] - 0s 39ms/step - loss: 0.0134 - val_loss: 0.0496 - lr: 2.6214e-04\n","Epoch 161/270\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0140 - val_loss: 0.0495 - lr: 2.6214e-04\n","Epoch 162/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0132 - val_loss: 0.0501 - lr: 2.6214e-04\n","Epoch 163/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0133 - val_loss: 0.0496 - lr: 2.6214e-04\n","Epoch 164/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0136 - val_loss: 0.0502 - lr: 2.6214e-04\n","Epoch 165/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0131 - val_loss: 0.0501 - lr: 2.6214e-04\n","Epoch 166/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0144 - val_loss: 0.0496 - lr: 2.6214e-04\n","Epoch 167/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0129 - val_loss: 0.0497 - lr: 2.6214e-04\n","Epoch 168/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0144 - val_loss: 0.0501 - lr: 2.6214e-04\n","Epoch 169/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0131 - val_loss: 0.0498 - lr: 2.6214e-04\n","Epoch 170/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0132 - val_loss: 0.0508 - lr: 2.6214e-04\n","Epoch 171/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0135 - val_loss: 0.0504 - lr: 2.6214e-04\n","Epoch 172/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0138 - val_loss: 0.0502 - lr: 2.6214e-04\n","Epoch 173/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0128 - val_loss: 0.0497 - lr: 2.0972e-04\n","Epoch 174/270\n","13/13 [==============================] - 1s 55ms/step - loss: 0.0125 - val_loss: 0.0495 - lr: 2.0972e-04\n","Epoch 175/270\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0137 - val_loss: 0.0498 - lr: 2.0972e-04\n","Epoch 176/270\n","13/13 [==============================] - 1s 57ms/step - loss: 0.0124 - val_loss: 0.0495 - lr: 2.0972e-04\n","Epoch 177/270\n","13/13 [==============================] - 1s 57ms/step - loss: 0.0130 - val_loss: 0.0498 - lr: 2.0972e-04\n","Epoch 178/270\n","13/13 [==============================] - 1s 42ms/step - loss: 0.0138 - val_loss: 0.0503 - lr: 2.0972e-04\n","Epoch 179/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0124 - val_loss: 0.0498 - lr: 2.0972e-04\n","Epoch 180/270\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0127 - val_loss: 0.0502 - lr: 2.0972e-04\n","Epoch 181/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0123 - val_loss: 0.0499 - lr: 2.0972e-04\n","Epoch 182/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0124 - val_loss: 0.0501 - lr: 1.6777e-04\n","Epoch 183/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0119 - val_loss: 0.0494 - lr: 1.6777e-04\n","Epoch 184/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0124 - val_loss: 0.0499 - lr: 1.6777e-04\n","Epoch 185/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0125 - val_loss: 0.0493 - lr: 1.6777e-04\n","Epoch 186/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0119 - val_loss: 0.0493 - lr: 1.6777e-04\n","Epoch 187/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0121 - val_loss: 0.0494 - lr: 1.6777e-04\n","Epoch 188/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0122 - val_loss: 0.0500 - lr: 1.6777e-04\n","Epoch 189/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0120 - val_loss: 0.0493 - lr: 1.3422e-04\n","Epoch 190/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0123 - val_loss: 0.0494 - lr: 1.3422e-04\n","Epoch 191/270\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0122 - val_loss: 0.0490 - lr: 1.3422e-04\n","Epoch 192/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0113 - val_loss: 0.0493 - lr: 1.3422e-04\n","Epoch 193/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0116 - val_loss: 0.0495 - lr: 1.3422e-04\n","Epoch 194/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0119 - val_loss: 0.0495 - lr: 1.3422e-04\n","Epoch 195/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0114 - val_loss: 0.0493 - lr: 1.3422e-04\n","Epoch 196/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0123 - val_loss: 0.0492 - lr: 1.3422e-04\n","Epoch 197/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0114 - val_loss: 0.0491 - lr: 1.3422e-04\n","Epoch 198/270\n","13/13 [==============================] - 1s 48ms/step - loss: 0.0112 - val_loss: 0.0491 - lr: 1.0737e-04\n","Epoch 199/270\n","13/13 [==============================] - 1s 58ms/step - loss: 0.0118 - val_loss: 0.0490 - lr: 1.0737e-04\n","Epoch 200/270\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0116 - val_loss: 0.0494 - lr: 1.0737e-04\n","Epoch 201/270\n","13/13 [==============================] - 1s 49ms/step - loss: 0.0119 - val_loss: 0.0490 - lr: 1.0737e-04\n","Epoch 202/270\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0114 - val_loss: 0.0493 - lr: 1.0737e-04\n","Epoch 203/270\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0109 - val_loss: 0.0490 - lr: 1.0737e-04\n","Epoch 204/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0110 - val_loss: 0.0490 - lr: 1.0737e-04\n","Epoch 205/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0111 - val_loss: 0.0490 - lr: 1.0737e-04\n","Epoch 206/270\n","13/13 [==============================] - 0s 39ms/step - loss: 0.0120 - val_loss: 0.0489 - lr: 1.0737e-04\n","Epoch 207/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0106 - val_loss: 0.0490 - lr: 1.0737e-04\n","Epoch 208/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0118 - val_loss: 0.0491 - lr: 1.0737e-04\n","Epoch 209/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0114 - val_loss: 0.0491 - lr: 1.0737e-04\n","Epoch 210/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0110 - val_loss: 0.0489 - lr: 1.0737e-04\n","Epoch 211/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0115 - val_loss: 0.0490 - lr: 1.0737e-04\n","Epoch 212/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0117 - val_loss: 0.0491 - lr: 1.0737e-04\n","Epoch 213/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0106 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 214/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0109 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 215/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0106 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 216/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0117 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 217/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0110 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 218/270\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0108 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 219/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0107 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 220/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0105 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 221/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0108 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 222/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0116 - val_loss: 0.0493 - lr: 1.0000e-04\n","Epoch 223/270\n","13/13 [==============================] - 1s 42ms/step - loss: 0.0111 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 224/270\n","13/13 [==============================] - 1s 52ms/step - loss: 0.0114 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 225/270\n","13/13 [==============================] - 1s 57ms/step - loss: 0.0112 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 226/270\n","13/13 [==============================] - 1s 48ms/step - loss: 0.0105 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 227/270\n","13/13 [==============================] - 1s 60ms/step - loss: 0.0108 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 228/270\n","13/13 [==============================] - 1s 44ms/step - loss: 0.0103 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 229/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0113 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 230/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0111 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 231/270\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0107 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 232/270\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0114 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 233/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0114 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 234/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0103 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 235/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0121 - val_loss: 0.0493 - lr: 1.0000e-04\n","Epoch 236/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0109 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 237/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0107 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 238/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0109 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 239/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0104 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 240/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0107 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 241/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0108 - val_loss: 0.0492 - lr: 1.0000e-04\n","Epoch 242/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0112 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 243/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0110 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 244/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0104 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 245/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0105 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 246/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0104 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 247/270\n","13/13 [==============================] - 12s 988ms/step - loss: 0.0115 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 248/270\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0106 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 249/270\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0113 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 250/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0109 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 251/270\n","13/13 [==============================] - 1s 45ms/step - loss: 0.0108 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 252/270\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0105 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 253/270\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0112 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 254/270\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0098 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 255/270\n","13/13 [==============================] - 1s 52ms/step - loss: 0.0104 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 256/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0105 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 257/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0111 - val_loss: 0.0493 - lr: 1.0000e-04\n","Epoch 258/270\n","13/13 [==============================] - 12s 966ms/step - loss: 0.0104 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 259/270\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0105 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 260/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0104 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 261/270\n","13/13 [==============================] - 1s 38ms/step - loss: 0.0101 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 262/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0102 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 263/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0108 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 264/270\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0101 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 265/270\n","13/13 [==============================] - 1s 38ms/step - loss: 0.0098 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 266/270\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0106 - val_loss: 0.0492 - lr: 1.0000e-04\n","Epoch 267/270\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0108 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 268/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0104 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 269/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0099 - val_loss: 0.0494 - lr: 1.0000e-04\n","Epoch 270/270\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0102 - val_loss: 0.0489 - lr: 1.0000e-04\n"]}]},{"cell_type":"code","source":["#@title Sample Prediction with Best Weights\n","\n","from tensorflow.keras.models import load_model\n","'''either load or use existing model'''\n","modelcp = load_model('/content/best_checkpoint')\n","scroll = 32\n","predictions = modelcp.predict(V[::898//scroll])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlQRRXY_x-Ei","executionInfo":{"status":"ok","timestamp":1703887832788,"user_tz":-210,"elapsed":5123,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}},"outputId":"1c8284c7-b41d-408b-d28e-30fef0e3078f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 1s 139ms/step\n"]}]},{"cell_type":"code","source":["for n in range(scroll):\n","  print(f'Predicted:{np.round(predictions[n],3)}\\nExpected: {np.round(Yv[::898//scroll][n],3)} \\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvFbJPCBhar8","executionInfo":{"status":"ok","timestamp":1703887835759,"user_tz":-210,"elapsed":411,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}},"outputId":"28999d31-fc2a-40ec-dcb0-e1445c8354b3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted:[0.349 0.322]\n","Expected: [0.333 0.365] \n","\n","Predicted:[0.486 0.216]\n","Expected: [0.5   0.221] \n","\n","Predicted:[0.63 0.17]\n","Expected: [0.667 0.192] \n","\n","Predicted:[0.443 0.353]\n","Expected: [0.5   0.356] \n","\n","Predicted:[0.433 0.2  ]\n","Expected: [0.5   0.163] \n","\n","Predicted:[0.674 0.346]\n","Expected: [0.667 0.346] \n","\n","Predicted:[0.332 0.225]\n","Expected: [0.333 0.231] \n","\n","Predicted:[0.467 0.338]\n","Expected: [0.333 0.346] \n","\n","Predicted:[0.414 0.292]\n","Expected: [0.333 0.327] \n","\n","Predicted:[0.498 0.279]\n","Expected: [0.5   0.279] \n","\n","Predicted:[0.335 0.326]\n","Expected: [0.333 0.327] \n","\n","Predicted:[0.578 0.302]\n","Expected: [0.5   0.308] \n","\n","Predicted:[0.466 0.31 ]\n","Expected: [0.5   0.327] \n","\n","Predicted:[0.553 0.321]\n","Expected: [0.667 0.308] \n","\n","Predicted:[0.585 0.322]\n","Expected: [0.5   0.288] \n","\n","Predicted:[0.501 0.389]\n","Expected: [0.5   0.385] \n","\n","Predicted:[0.377 0.205]\n","Expected: [0.5   0.269] \n","\n","Predicted:[0.613 0.382]\n","Expected: [0.5   0.404] \n","\n","Predicted:[0.496 0.432]\n","Expected: [0.5   0.433] \n","\n","Predicted:[0.489 0.312]\n","Expected: [0.5   0.308] \n","\n","Predicted:[0.503 0.247]\n","Expected: [0.5  0.25] \n","\n","Predicted:[0.667 0.379]\n","Expected: [0.667 0.375] \n","\n","Predicted:[0.394 0.16 ]\n","Expected: [0.333 0.163] \n","\n","Predicted:[0.392 0.276]\n","Expected: [0.333 0.298] \n","\n","Predicted:[0.598 0.294]\n","Expected: [0.5   0.231] \n","\n","Predicted:[0.631 0.201]\n","Expected: [0.5   0.231] \n","\n","Predicted:[0.63  0.201]\n","Expected: [0.333 0.173] \n","\n","Predicted:[0.441 0.248]\n","Expected: [0.333 0.25 ] \n","\n","Predicted:[0.66  0.301]\n","Expected: [0.667 0.298] \n","\n","Predicted:[0.608 0.206]\n","Expected: [0.5   0.212] \n","\n","Predicted:[0.402 0.328]\n","Expected: [0.5  0.24] \n","\n","Predicted:[0.255 0.336]\n","Expected: [0.333 0.356] \n","\n"]}]}]}