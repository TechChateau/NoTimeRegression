{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNfMVUPeMQg9JQRKkSY/Wvq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#@title License and Disclaimer\n","\n","'''\n","The Wine Quality dataset is under CC0 License. More information below.\n","https://creativecommons.org/publicdomain/zero/1.0/\n","\n","The following is in respect of the software contributions of this project.\n","\n","Copyright <2023> <Parham Faraji>\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy of\n","this software and associated documentation files (the “Software”), to deal in\n","the Software without restriction, including without limitation the rights to\n","use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n","the Software, and to permit persons to whom the Software is furnished to do so,\n","subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n","FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n","COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n","IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n","CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","'''"],"metadata":{"id":"YX_hZYC8Mp_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pKjLoTZ7UQKQ"},"outputs":[],"source":["#@title Data Import\n","\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","# Construct a tf.data.Dataset\n","train = tfds.load('wine_quality/white', split='train', shuffle_files=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKSGDdUIelu4"},"outputs":[],"source":["#@title Packaging Function\n","\n","import numpy as np\n","import cv2\n","from tqdm import tqdm\n","\n","def normalize(cluster):\n","  clusterNorm = np.zeros_like(cluster)\n","\n","  for f in range(cluster.shape[1]):\n","    offset = cluster[:,f] - np.min(cluster[:,f])\n","    clusterNorm[:,f] = offset / np.max(offset)\n","    del offset\n","\n","  print(' min:', np.min(clusterNorm, axis=0), '\\n max:',\n","                 np.max(clusterNorm, axis=0), '\\n')\n","  del cluster\n","  return np.reshape(clusterNorm, [clusterNorm.shape[0],\n","                                  clusterNorm.shape[1], 1])\n","\n","def package(data, count):\n","  # dataset = dataset.take(3)\n","  # list(dataset.as_numpy_iterator())\n","  given = []\n","  regress = []\n","  for sample in tqdm(data.take(count)):\n","\n","    features = sample['features']\n","\n","    given.append([\n","    features['alcohol'],\n","    features['chlorides'],\n","    features['citric acid'],\n","    features['density'],\n","    features['free sulfur dioxide'],\n","    features['pH'],\n","    features['residual sugar'],\n","    features['sulphates'],\n","    features['total sulfur dioxide'],\n","    features['volatile acidity']\n","    ])\n","\n","    regress.append([\n","    sample['quality'],\n","    features['fixed acidity']\n","    ])\n","\n","  given_out = np.array(given)\n","  regress_out = np.array(regress)\n","  del given, regress\n","  print(given_out.shape, regress_out.shape, '\\n')\n","  return given_out, regress_out\n"]},{"cell_type":"code","source":["#@title Packaging Data\n","# del V, Yv, X, Yx\n","giv, reg = package(train, 4898) #4898\n","giv = normalize(giv)\n","reg = normalize(reg)\n","\n","split = 898\n","X, Yx, V, Yv= giv[split:], reg[split:], giv[:split], reg[:split]\n","\n","Yv = np.reshape(Yv, [Yv.shape[0], Yv.shape[1]])\n","Yx = np.reshape(Yx, [Yx.shape[0], Yx.shape[1]])\n","\n","print(X.shape, V.shape)\n","print(Yx.shape, Yv.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUHvhAL77kRe","executionInfo":{"status":"ok","timestamp":1703694250894,"user_tz":-210,"elapsed":3814,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}},"outputId":"0a80a17e-5e02-4bdf-ac94-16a5c8b200a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4898/4898 [00:02<00:00, 1707.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(4898, 10) (4898, 2) \n","\n"," min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n"," max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] \n","\n"," min: [0. 0.] \n"," max: [1. 1.] \n","\n","(4000, 10, 1) (898, 10, 1)\n","(4000, 2) (898, 2)\n"]}]},{"cell_type":"code","source":["#@title Layer Designer (1D) Function\n","\n","from tensorflow import keras\n","\n","# This approach helps make design intuitive, intelligible, and easy to alter\n","# No need to worry about the order of operations\n","# You simply enable certain sub-layers and focus on the grand architecture\n","# It is especially useful as a tool for beginners to start from the middle by fiddling around\n","# The \"engage\" variable in the function is responsible for enabling all operations needed for a composite layer\n","# Using default parameters, layers can be on autopilot by verifying input/\"consign\" alone\n","def layer(consign, engage=[1,0,0,1,0,0], fmaps=24, k=3, act='relu', step=1, pool=2, scale=2, dp=.20, ep=1e-5, pad='same'): # same/valid\n","\n","  # preset-choices:\n","  if engage[0]==1:\n","    consign = keras.layers.Conv1D(filters=fmaps, kernel_size=k, strides=step, padding=pad)(consign)\n","  if engage[0]==2:\n","    consign = keras.layers.Conv1DTranspose(filters=fmaps, kernel_size=k, strides=step, padding=pad)(consign)\n","  if engage[0]==3:\n","    attn = keras.layers.MultiHeadAttention(num_heads=3, key_dim=fmaps)\n","    consign = attn(consign, consign)\n","\n","  if engage[1]:\n","    consign = keras.layers.BatchNormalization()(consign)\n","  if engage[2]:\n","    consign = keras.layers.LayerNormalization(epsilon=ep)(consign)\n","  if engage[3]:\n","    consign = keras.layers.Activation(act)(consign)\n","  if engage[4]:\n","    consign = keras.layers.MaxPooling1D(pool)(consign)\n","  if engage[5]:\n","    consign = keras.layers.UpSampling1D(scale)(consign)\n","  if engage[6]:\n","    consign = keras.layers.Dropout(dp)(consign)\n","\n","  return consign\n"],"metadata":{"id":"ah2k-PVTUxau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 1D Regression Model Demo\n","\n","'''1-dimensionals are cost efficient, so here we went with a depth of 7-8 layers, and width of 3-4, with as much demonstrated diversity'''\n","def architect(nc=2): # nc = number of output values\n","\n","  # lunch\n","  inputshape = [10,1]\n","  inputlayer = keras.layers.Input(inputshape)\n","\n","  # block a @lvl1\n","  a = layer(inputlayer, [1, 1,0, 1, 0,0,0], fmaps=30, k=3, act='relu')\n","  a = layer(a,          [1, 1,0, 1, 0,0,0], fmaps=30, k=3, act='relu')\n","  a = layer(a,          [1, 1,0, 1, 0,0,0], fmaps=30, k=2, act='relu')\n","\n","  # block b @lvl1\n","  b = layer(inputlayer, [0, 0,0, 0, 0,1,0], scale=2)\n","  b = layer(b,          [1, 1,0, 1, 0,0,0], fmaps=30, k=5, act='relu', pad='valid')\n","  b = layer(b,          [1, 1,0, 1, 0,0,1], fmaps=30, k=4, act='relu', pad='valid')\n","  b = layer(b,          [1, 1,0, 1, 0,0,0], fmaps=30, k=4, act='relu', pad='valid')\n","\n","  # block c @lvl1\n","  c = layer(inputlayer, [2, 0,0, 1, 0,0,0], fmaps=30, k=4, step=2, act='relu')\n","  c = layer(c,          [1, 0,0, 1, 0,0,0], fmaps=30, k=4, act='relu')\n","  c = layer(c,          [1, 0,0, 1, 1,0,0], fmaps=30, k=4, pool=2, act='relu')\n","\n","  # res=inputlayer\n","\n","  # merge\n","  con = keras.layers.Concatenate()([inputlayer, a, b, c])\n","  con = keras.layers.BatchNormalization()(con)\n","\n","  # attention bypass\n","  a2 = layer(b,     [3, 0,1, 0, 0,0,0], fmaps=30)\n","  a2 = layer(a2,    [1, 1,0, 1, 0,0,1], fmaps=20, k=3, act='relu')\n","\n","  # 2nd lvl independent layers\n","  b2 = layer(con,   [1, 0,0, 1, 0,0,0], fmaps=40, k=5, act='relu')\n","\n","  c2 = layer(con,   [1, 1,0, 1, 0,0,0], fmaps=40, k=4, act='relu')\n","\n","  d2 = layer(con,   [2, 1,0, 1, 0,0,0], fmaps=40, k=5, act='relu')\n","\n","  # res 2\n","  res2 = layer(con, [1, 1,0, 1, 0,0,0], fmaps=30, k=1, act='linear')\n","\n","  # merge 2\n","  con2 = keras.layers.Concatenate()([res2, a2, b2, c2, d2])\n","  con2 = keras.layers.BatchNormalization()(con2)\n","\n","  # 3rd lvl independent layers\n","  a3 = layer(con2, [2, 1,0, 1, 0,0,0], fmaps=40, k=5, act='relu')\n","\n","  b3 = layer(con2, [1, 1,0, 1, 0,0,0], fmaps=40, k=5, act='relu')\n","\n","  c3 = layer(con2, [1, 1,0, 1, 0,0,0], fmaps=40, k=4, act='relu')\n","\n","  d3 = layer(b3,   [1, 0,0, 1, 0,0,1], fmaps=40, k=4, act='relu')\n","\n","  # res 3\n","  res3 = layer(con2, [1, 1,0, 1, 0,0,0], fmaps=30, k=1, act='linear')\n","\n","  # merge 3\n","  con3 = keras.layers.Concatenate()([res3, a3, b3, c3, d3])\n","  con3 = keras.layers.BatchNormalization()(con3)\n","\n","  # finalization\n","  f = layer(con3, [1, 1,0, 1, 0,0,0], fmaps=50, k=4, act='relu')\n","  f = layer(f,    [1, 1,0, 1, 0,0,0], fmaps=40, k=3, act='relu')\n","\n","  # res 4\n","  res4 = layer(con3, [1, 1,0, 1, 0,0,0], fmaps=40, k=1, act='linear')\n","\n","  # merge 4\n","  con4 = keras.layers.Concatenate()([res4, f])\n","  con4 = keras.layers.BatchNormalization()(con4)\n","\n","  gap = keras.layers.GlobalAveragePooling1D()(con4)\n","  outputlayer = keras.layers.Dense(nc, activation='sigmoid')(gap)\n","\n","  return inputlayer, outputlayer\n","\n","\n","# tie regression model\n","input, output = architect()\n","model = keras.models.Model(inputs=input, outputs=output)\n","\n","# compile model\n","model.compile(loss='mae', optimizer=keras.optimizers.Adam())\n","\n","# print model\n","model.summary()\n","\n","# implement callbacks\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8,\n","                patience=4, min_lr=1e-4)\n","\n","es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto',\n","                verbose=1, patience=35, restore_best_weights=True)\n","\n","cp = keras.callbacks.ModelCheckpoint(\n","                './best_checkpoint', monitor=\"val_loss\", verbose=0,\n","                save_best_only=True, save_weights_only=False, mode=\"auto\",\n","                save_freq=\"epoch\", initial_value_threshold=0.0487) # *threshold based on a previous test run\n","\n","# fit data into model\n","model.fit(x=X, y=Yx, batch_size=308, epochs=270, verbose=1, # *batch size based on available hardware\n","                validation_data=(V, Yv), shuffle=True,\n","                callbacks=[reduce_lr, es, cp]) # or [reduce_lr, es]\n","\n","# Save model\n","model.save('NTR')"],"metadata":{"id":"cBvRNmChlL3j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703697768528,"user_tz":-210,"elapsed":166513,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}},"outputId":"d7514ed2-b1aa-46c9-acf1-dad24fd35d93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_18\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_20 (InputLayer)       [(None, 10, 1)]              0         []                            \n","                                                                                                  \n"," up_sampling1d_18 (UpSampli  (None, 20, 1)                0         ['input_20[0][0]']            \n"," ng1D)                                                                                            \n","                                                                                                  \n"," conv1d_343 (Conv1D)         (None, 16, 30)               180       ['up_sampling1d_18[0][0]']    \n","                                                                                                  \n"," batch_normalization_319 (B  (None, 16, 30)               120       ['conv1d_343[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," conv1d_340 (Conv1D)         (None, 10, 30)               120       ['input_20[0][0]']            \n","                                                                                                  \n"," activation_397 (Activation  (None, 16, 30)               0         ['batch_normalization_319[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," batch_normalization_316 (B  (None, 10, 30)               120       ['conv1d_340[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," conv1d_344 (Conv1D)         (None, 13, 30)               3630      ['activation_397[0][0]']      \n","                                                                                                  \n"," activation_394 (Activation  (None, 10, 30)               0         ['batch_normalization_316[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," batch_normalization_320 (B  (None, 13, 30)               120       ['conv1d_344[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," conv1d_transpose_54 (Conv1  (None, 20, 30)               150       ['input_20[0][0]']            \n"," DTranspose)                                                                                      \n","                                                                                                  \n"," conv1d_341 (Conv1D)         (None, 10, 30)               2730      ['activation_394[0][0]']      \n","                                                                                                  \n"," activation_398 (Activation  (None, 13, 30)               0         ['batch_normalization_320[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," activation_400 (Activation  (None, 20, 30)               0         ['conv1d_transpose_54[0][0]'] \n"," )                                                                                                \n","                                                                                                  \n"," batch_normalization_317 (B  (None, 10, 30)               120       ['conv1d_341[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," dropout_21 (Dropout)        (None, 13, 30)               0         ['activation_398[0][0]']      \n","                                                                                                  \n"," conv1d_346 (Conv1D)         (None, 20, 30)               3630      ['activation_400[0][0]']      \n","                                                                                                  \n"," activation_395 (Activation  (None, 10, 30)               0         ['batch_normalization_317[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," conv1d_345 (Conv1D)         (None, 10, 30)               3630      ['dropout_21[0][0]']          \n","                                                                                                  \n"," activation_401 (Activation  (None, 20, 30)               0         ['conv1d_346[0][0]']          \n"," )                                                                                                \n","                                                                                                  \n"," conv1d_342 (Conv1D)         (None, 10, 30)               1830      ['activation_395[0][0]']      \n","                                                                                                  \n"," batch_normalization_321 (B  (None, 10, 30)               120       ['conv1d_345[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," conv1d_347 (Conv1D)         (None, 20, 30)               3630      ['activation_401[0][0]']      \n","                                                                                                  \n"," batch_normalization_318 (B  (None, 10, 30)               120       ['conv1d_342[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," activation_399 (Activation  (None, 10, 30)               0         ['batch_normalization_321[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," activation_402 (Activation  (None, 20, 30)               0         ['conv1d_347[0][0]']          \n"," )                                                                                                \n","                                                                                                  \n"," activation_396 (Activation  (None, 10, 30)               0         ['batch_normalization_318[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," max_pooling1d_18 (MaxPooli  (None, 10, 30)               0         ['activation_402[0][0]']      \n"," ng1D)                                                                                            \n","                                                                                                  \n"," multi_head_attention_18 (M  (None, 10, 30)               11100     ['activation_399[0][0]',      \n"," ultiHeadAttention)                                                  'activation_399[0][0]']      \n","                                                                                                  \n"," concatenate_68 (Concatenat  (None, 10, 91)               0         ['input_20[0][0]',            \n"," e)                                                                  'activation_396[0][0]',      \n","                                                                     'activation_399[0][0]',      \n","                                                                     'max_pooling1d_18[0][0]']    \n","                                                                                                  \n"," layer_normalization_18 (La  (None, 10, 30)               60        ['multi_head_attention_18[0][0\n"," yerNormalization)                                                  ]']                           \n","                                                                                                  \n"," batch_normalization_322 (B  (None, 10, 91)               364       ['concatenate_68[0][0]']      \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," conv1d_348 (Conv1D)         (None, 10, 20)               1820      ['layer_normalization_18[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv1d_351 (Conv1D)         (None, 10, 30)               2760      ['batch_normalization_322[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," batch_normalization_323 (B  (None, 10, 20)               80        ['conv1d_348[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," conv1d_350 (Conv1D)         (None, 10, 40)               14600     ['batch_normalization_322[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," conv1d_transpose_55 (Conv1  (None, 10, 40)               18240     ['batch_normalization_322[0][0\n"," DTranspose)                                                        ]']                           \n","                                                                                                  \n"," batch_normalization_326 (B  (None, 10, 30)               120       ['conv1d_351[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," activation_403 (Activation  (None, 10, 20)               0         ['batch_normalization_323[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," conv1d_349 (Conv1D)         (None, 10, 40)               18240     ['batch_normalization_322[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," batch_normalization_324 (B  (None, 10, 40)               160       ['conv1d_350[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," batch_normalization_325 (B  (None, 10, 40)               160       ['conv1d_transpose_55[0][0]'] \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," activation_407 (Activation  (None, 10, 30)               0         ['batch_normalization_326[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," dropout_22 (Dropout)        (None, 10, 20)               0         ['activation_403[0][0]']      \n","                                                                                                  \n"," activation_404 (Activation  (None, 10, 40)               0         ['conv1d_349[0][0]']          \n"," )                                                                                                \n","                                                                                                  \n"," activation_405 (Activation  (None, 10, 40)               0         ['batch_normalization_324[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," activation_406 (Activation  (None, 10, 40)               0         ['batch_normalization_325[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," concatenate_69 (Concatenat  (None, 10, 170)              0         ['activation_407[0][0]',      \n"," e)                                                                  'dropout_22[0][0]',          \n","                                                                     'activation_404[0][0]',      \n","                                                                     'activation_405[0][0]',      \n","                                                                     'activation_406[0][0]']      \n","                                                                                                  \n"," batch_normalization_327 (B  (None, 10, 170)              680       ['concatenate_69[0][0]']      \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," conv1d_352 (Conv1D)         (None, 10, 40)               34040     ['batch_normalization_327[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," batch_normalization_329 (B  (None, 10, 40)               160       ['conv1d_352[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," activation_409 (Activation  (None, 10, 40)               0         ['batch_normalization_329[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," conv1d_355 (Conv1D)         (None, 10, 30)               5130      ['batch_normalization_327[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," conv1d_transpose_56 (Conv1  (None, 10, 40)               34040     ['batch_normalization_327[0][0\n"," DTranspose)                                                        ]']                           \n","                                                                                                  \n"," conv1d_353 (Conv1D)         (None, 10, 40)               27240     ['batch_normalization_327[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," conv1d_354 (Conv1D)         (None, 10, 40)               6440      ['activation_409[0][0]']      \n","                                                                                                  \n"," batch_normalization_331 (B  (None, 10, 30)               120       ['conv1d_355[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," batch_normalization_328 (B  (None, 10, 40)               160       ['conv1d_transpose_56[0][0]'] \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," batch_normalization_330 (B  (None, 10, 40)               160       ['conv1d_353[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," activation_411 (Activation  (None, 10, 40)               0         ['conv1d_354[0][0]']          \n"," )                                                                                                \n","                                                                                                  \n"," activation_412 (Activation  (None, 10, 30)               0         ['batch_normalization_331[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," activation_408 (Activation  (None, 10, 40)               0         ['batch_normalization_328[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," activation_410 (Activation  (None, 10, 40)               0         ['batch_normalization_330[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," dropout_23 (Dropout)        (None, 10, 40)               0         ['activation_411[0][0]']      \n","                                                                                                  \n"," concatenate_70 (Concatenat  (None, 10, 190)              0         ['activation_412[0][0]',      \n"," e)                                                                  'activation_408[0][0]',      \n","                                                                     'activation_409[0][0]',      \n","                                                                     'activation_410[0][0]',      \n","                                                                     'dropout_23[0][0]']          \n","                                                                                                  \n"," batch_normalization_332 (B  (None, 10, 190)              760       ['concatenate_70[0][0]']      \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," conv1d_356 (Conv1D)         (None, 10, 50)               38050     ['batch_normalization_332[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," batch_normalization_333 (B  (None, 10, 50)               200       ['conv1d_356[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," activation_413 (Activation  (None, 10, 50)               0         ['batch_normalization_333[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," conv1d_358 (Conv1D)         (None, 10, 40)               7640      ['batch_normalization_332[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," conv1d_357 (Conv1D)         (None, 10, 40)               6040      ['activation_413[0][0]']      \n","                                                                                                  \n"," batch_normalization_335 (B  (None, 10, 40)               160       ['conv1d_358[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," batch_normalization_334 (B  (None, 10, 40)               160       ['conv1d_357[0][0]']          \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," activation_415 (Activation  (None, 10, 40)               0         ['batch_normalization_335[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," activation_414 (Activation  (None, 10, 40)               0         ['batch_normalization_334[0][0\n"," )                                                                  ]']                           \n","                                                                                                  \n"," concatenate_71 (Concatenat  (None, 10, 80)               0         ['activation_415[0][0]',      \n"," e)                                                                  'activation_414[0][0]']      \n","                                                                                                  \n"," batch_normalization_336 (B  (None, 10, 80)               320       ['concatenate_71[0][0]']      \n"," atchNormalization)                                                                               \n","                                                                                                  \n"," global_average_pooling1d_1  (None, 80)                   0         ['batch_normalization_336[0][0\n"," 8 (GlobalAveragePooling1D)                                         ]']                           \n","                                                                                                  \n"," dense_18 (Dense)            (None, 2)                    162       ['global_average_pooling1d_18[\n","                                                                    0][0]']                       \n","                                                                                                  \n","==================================================================================================\n","Total params: 249616 (975.06 KB)\n","Trainable params: 247374 (966.30 KB)\n","Non-trainable params: 2242 (8.76 KB)\n","__________________________________________________________________________________________________\n","Epoch 1/250\n","13/13 [==============================] - 18s 112ms/step - loss: 0.1467 - val_loss: 0.1691 - lr: 0.0010\n","Epoch 2/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.1077 - val_loss: 0.1489 - lr: 0.0010\n","Epoch 3/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0925 - val_loss: 0.1358 - lr: 0.0010\n","Epoch 4/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0800 - val_loss: 0.1110 - lr: 0.0010\n","Epoch 5/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0754 - val_loss: 0.1010 - lr: 0.0010\n","Epoch 6/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0730 - val_loss: 0.0970 - lr: 0.0010\n","Epoch 7/250\n","13/13 [==============================] - 0s 32ms/step - loss: 0.0767 - val_loss: 0.1116 - lr: 0.0010\n","Epoch 8/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0705 - val_loss: 0.1007 - lr: 0.0010\n","Epoch 9/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0671 - val_loss: 0.0972 - lr: 0.0010\n","Epoch 10/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0652 - val_loss: 0.0975 - lr: 0.0010\n","Epoch 11/250\n","13/13 [==============================] - 0s 32ms/step - loss: 0.0636 - val_loss: 0.0986 - lr: 0.0010\n","Epoch 12/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0618 - val_loss: 0.1022 - lr: 0.0010\n","Epoch 13/250\n","13/13 [==============================] - 0s 33ms/step - loss: 0.0596 - val_loss: 0.1026 - lr: 0.0010\n","Epoch 14/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0592 - val_loss: 0.1101 - lr: 0.0010\n","Epoch 15/250\n","13/13 [==============================] - 1s 47ms/step - loss: 0.0578 - val_loss: 0.1083 - lr: 0.0010\n","Epoch 16/250\n","13/13 [==============================] - 1s 59ms/step - loss: 0.0566 - val_loss: 0.1055 - lr: 0.0010\n","Epoch 17/250\n","13/13 [==============================] - 1s 47ms/step - loss: 0.0553 - val_loss: 0.1085 - lr: 0.0010\n","Epoch 18/250\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0548 - val_loss: 0.1058 - lr: 0.0010\n","Epoch 19/250\n","13/13 [==============================] - 1s 49ms/step - loss: 0.0543 - val_loss: 0.1039 - lr: 0.0010\n","Epoch 20/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0545 - val_loss: 0.1001 - lr: 0.0010\n","Epoch 21/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0538 - val_loss: 0.1091 - lr: 0.0010\n","Epoch 22/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0529 - val_loss: 0.1066 - lr: 0.0010\n","Epoch 23/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0525 - val_loss: 0.1040 - lr: 0.0010\n","Epoch 24/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0529 - val_loss: 0.0962 - lr: 0.0010\n","Epoch 25/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0529 - val_loss: 0.1051 - lr: 0.0010\n","Epoch 26/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0518 - val_loss: 0.1012 - lr: 0.0010\n","Epoch 27/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0508 - val_loss: 0.1077 - lr: 0.0010\n","Epoch 28/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0499 - val_loss: 0.1022 - lr: 0.0010\n","Epoch 29/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0496 - val_loss: 0.0959 - lr: 0.0010\n","Epoch 30/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0490 - val_loss: 0.0981 - lr: 0.0010\n","Epoch 31/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0502 - val_loss: 0.0879 - lr: 0.0010\n","Epoch 32/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0503 - val_loss: 0.0934 - lr: 0.0010\n","Epoch 33/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0497 - val_loss: 0.0950 - lr: 0.0010\n","Epoch 34/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0485 - val_loss: 0.0895 - lr: 0.0010\n","Epoch 35/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0467 - val_loss: 0.0877 - lr: 0.0010\n","Epoch 36/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0461 - val_loss: 0.0899 - lr: 0.0010\n","Epoch 37/250\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0451 - val_loss: 0.0847 - lr: 0.0010\n","Epoch 38/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0458 - val_loss: 0.0920 - lr: 0.0010\n","Epoch 39/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0457 - val_loss: 0.0888 - lr: 0.0010\n","Epoch 40/250\n","13/13 [==============================] - 1s 57ms/step - loss: 0.0458 - val_loss: 0.0852 - lr: 0.0010\n","Epoch 41/250\n","13/13 [==============================] - 1s 54ms/step - loss: 0.0448 - val_loss: 0.0813 - lr: 0.0010\n","Epoch 42/250\n","13/13 [==============================] - 1s 55ms/step - loss: 0.0442 - val_loss: 0.0887 - lr: 0.0010\n","Epoch 43/250\n","13/13 [==============================] - 1s 55ms/step - loss: 0.0430 - val_loss: 0.0792 - lr: 0.0010\n","Epoch 44/250\n","13/13 [==============================] - 1s 42ms/step - loss: 0.0430 - val_loss: 0.0802 - lr: 0.0010\n","Epoch 45/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0427 - val_loss: 0.0827 - lr: 0.0010\n","Epoch 46/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0426 - val_loss: 0.0716 - lr: 0.0010\n","Epoch 47/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0429 - val_loss: 0.0734 - lr: 0.0010\n","Epoch 48/250\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0423 - val_loss: 0.0709 - lr: 0.0010\n","Epoch 49/250\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0418 - val_loss: 0.0705 - lr: 0.0010\n","Epoch 50/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0404 - val_loss: 0.0656 - lr: 0.0010\n","Epoch 51/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0398 - val_loss: 0.0648 - lr: 0.0010\n","Epoch 52/250\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0397 - val_loss: 0.0627 - lr: 0.0010\n","Epoch 53/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0394 - val_loss: 0.0665 - lr: 0.0010\n","Epoch 54/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0400 - val_loss: 0.0641 - lr: 0.0010\n","Epoch 55/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0397 - val_loss: 0.0632 - lr: 0.0010\n","Epoch 56/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0388 - val_loss: 0.0635 - lr: 0.0010\n","Epoch 57/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0372 - val_loss: 0.0603 - lr: 0.0010\n","Epoch 58/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0373 - val_loss: 0.0611 - lr: 0.0010\n","Epoch 59/250\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0382 - val_loss: 0.0585 - lr: 0.0010\n","Epoch 60/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0377 - val_loss: 0.0592 - lr: 0.0010\n","Epoch 61/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0368 - val_loss: 0.0598 - lr: 0.0010\n","Epoch 62/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0360 - val_loss: 0.0586 - lr: 0.0010\n","Epoch 63/250\n","13/13 [==============================] - 1s 47ms/step - loss: 0.0357 - val_loss: 0.0596 - lr: 0.0010\n","Epoch 64/250\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0356 - val_loss: 0.0605 - lr: 0.0010\n","Epoch 65/250\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0351 - val_loss: 0.0595 - lr: 0.0010\n","Epoch 66/250\n","13/13 [==============================] - 1s 60ms/step - loss: 0.0348 - val_loss: 0.0579 - lr: 0.0010\n","Epoch 67/250\n","13/13 [==============================] - 1s 48ms/step - loss: 0.0351 - val_loss: 0.0566 - lr: 0.0010\n","Epoch 68/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0352 - val_loss: 0.0589 - lr: 0.0010\n","Epoch 69/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0346 - val_loss: 0.0578 - lr: 0.0010\n","Epoch 70/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0337 - val_loss: 0.0584 - lr: 0.0010\n","Epoch 71/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0333 - val_loss: 0.0580 - lr: 0.0010\n","Epoch 72/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0330 - val_loss: 0.0635 - lr: 0.0010\n","Epoch 73/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0321 - val_loss: 0.0598 - lr: 0.0010\n","Epoch 74/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0325 - val_loss: 0.0623 - lr: 0.0010\n","Epoch 75/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0327 - val_loss: 0.0637 - lr: 0.0010\n","Epoch 76/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0322 - val_loss: 0.0591 - lr: 0.0010\n","Epoch 77/250\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0323 - val_loss: 0.0600 - lr: 0.0010\n","Epoch 78/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0309 - val_loss: 0.0574 - lr: 8.0000e-04\n","Epoch 79/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0302 - val_loss: 0.0582 - lr: 8.0000e-04\n","Epoch 80/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0287 - val_loss: 0.0594 - lr: 8.0000e-04\n","Epoch 81/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0285 - val_loss: 0.0590 - lr: 8.0000e-04\n","Epoch 82/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0284 - val_loss: 0.0559 - lr: 8.0000e-04\n","Epoch 83/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0280 - val_loss: 0.0571 - lr: 8.0000e-04\n","Epoch 84/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0279 - val_loss: 0.0590 - lr: 8.0000e-04\n","Epoch 85/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0279 - val_loss: 0.0575 - lr: 8.0000e-04\n","Epoch 86/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0277 - val_loss: 0.0566 - lr: 8.0000e-04\n","Epoch 87/250\n","13/13 [==============================] - 0s 39ms/step - loss: 0.0276 - val_loss: 0.0576 - lr: 8.0000e-04\n","Epoch 88/250\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0278 - val_loss: 0.0588 - lr: 8.0000e-04\n","Epoch 89/250\n","13/13 [==============================] - 1s 52ms/step - loss: 0.0275 - val_loss: 0.0569 - lr: 8.0000e-04\n","Epoch 90/250\n","13/13 [==============================] - 1s 49ms/step - loss: 0.0270 - val_loss: 0.0543 - lr: 8.0000e-04\n","Epoch 91/250\n","13/13 [==============================] - 1s 59ms/step - loss: 0.0272 - val_loss: 0.0581 - lr: 8.0000e-04\n","Epoch 92/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0262 - val_loss: 0.0572 - lr: 8.0000e-04\n","Epoch 93/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0258 - val_loss: 0.0564 - lr: 8.0000e-04\n","Epoch 94/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0257 - val_loss: 0.0546 - lr: 8.0000e-04\n","Epoch 95/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0264 - val_loss: 0.0565 - lr: 8.0000e-04\n","Epoch 96/250\n","13/13 [==============================] - 1s 45ms/step - loss: 0.0264 - val_loss: 0.0539 - lr: 8.0000e-04\n","Epoch 97/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0262 - val_loss: 0.0538 - lr: 8.0000e-04\n","Epoch 98/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0245 - val_loss: 0.0547 - lr: 6.4000e-04\n","Epoch 99/250\n","13/13 [==============================] - 1s 44ms/step - loss: 0.0239 - val_loss: 0.0534 - lr: 6.4000e-04\n","Epoch 100/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0229 - val_loss: 0.0547 - lr: 6.4000e-04\n","Epoch 101/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0224 - val_loss: 0.0542 - lr: 6.4000e-04\n","Epoch 102/250\n","13/13 [==============================] - 1s 46ms/step - loss: 0.0233 - val_loss: 0.0533 - lr: 6.4000e-04\n","Epoch 103/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0224 - val_loss: 0.0531 - lr: 6.4000e-04\n","Epoch 104/250\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0236 - val_loss: 0.0528 - lr: 6.4000e-04\n","Epoch 105/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0231 - val_loss: 0.0535 - lr: 6.4000e-04\n","Epoch 106/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0216 - val_loss: 0.0537 - lr: 5.1200e-04\n","Epoch 107/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0209 - val_loss: 0.0529 - lr: 5.1200e-04\n","Epoch 108/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0210 - val_loss: 0.0525 - lr: 5.1200e-04\n","Epoch 109/250\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0210 - val_loss: 0.0523 - lr: 5.1200e-04\n","Epoch 110/250\n","13/13 [==============================] - 1s 42ms/step - loss: 0.0205 - val_loss: 0.0517 - lr: 5.1200e-04\n","Epoch 111/250\n","13/13 [==============================] - 1s 46ms/step - loss: 0.0202 - val_loss: 0.0523 - lr: 5.1200e-04\n","Epoch 112/250\n","13/13 [==============================] - 1s 48ms/step - loss: 0.0204 - val_loss: 0.0527 - lr: 5.1200e-04\n","Epoch 113/250\n","13/13 [==============================] - 1s 60ms/step - loss: 0.0199 - val_loss: 0.0517 - lr: 5.1200e-04\n","Epoch 114/250\n","13/13 [==============================] - 1s 49ms/step - loss: 0.0206 - val_loss: 0.0527 - lr: 5.1200e-04\n","Epoch 115/250\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0208 - val_loss: 0.0528 - lr: 5.1200e-04\n","Epoch 116/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0205 - val_loss: 0.0515 - lr: 5.1200e-04\n","Epoch 117/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0198 - val_loss: 0.0524 - lr: 5.1200e-04\n","Epoch 118/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0197 - val_loss: 0.0518 - lr: 5.1200e-04\n","Epoch 119/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0196 - val_loss: 0.0520 - lr: 5.1200e-04\n","Epoch 120/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0191 - val_loss: 0.0509 - lr: 5.1200e-04\n","Epoch 121/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0190 - val_loss: 0.0515 - lr: 5.1200e-04\n","Epoch 122/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0188 - val_loss: 0.0511 - lr: 5.1200e-04\n","Epoch 123/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0197 - val_loss: 0.0521 - lr: 5.1200e-04\n","Epoch 124/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0185 - val_loss: 0.0508 - lr: 5.1200e-04\n","Epoch 125/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0186 - val_loss: 0.0511 - lr: 5.1200e-04\n","Epoch 126/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0192 - val_loss: 0.0505 - lr: 5.1200e-04\n","Epoch 127/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0190 - val_loss: 0.0522 - lr: 5.1200e-04\n","Epoch 128/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0195 - val_loss: 0.0517 - lr: 5.1200e-04\n","Epoch 129/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0181 - val_loss: 0.0518 - lr: 4.0960e-04\n","Epoch 130/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0184 - val_loss: 0.0513 - lr: 4.0960e-04\n","Epoch 131/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0174 - val_loss: 0.0511 - lr: 4.0960e-04\n","Epoch 132/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0173 - val_loss: 0.0505 - lr: 4.0960e-04\n","Epoch 133/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0177 - val_loss: 0.0506 - lr: 4.0960e-04\n","Epoch 134/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0174 - val_loss: 0.0508 - lr: 4.0960e-04\n","Epoch 135/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0176 - val_loss: 0.0506 - lr: 4.0960e-04\n","Epoch 136/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0174 - val_loss: 0.0511 - lr: 4.0960e-04\n","Epoch 137/250\n","13/13 [==============================] - 1s 58ms/step - loss: 0.0164 - val_loss: 0.0507 - lr: 3.2768e-04\n","Epoch 138/250\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0171 - val_loss: 0.0508 - lr: 3.2768e-04\n","Epoch 139/250\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0160 - val_loss: 0.0505 - lr: 3.2768e-04\n","Epoch 140/250\n","13/13 [==============================] - 1s 51ms/step - loss: 0.0164 - val_loss: 0.0513 - lr: 3.2768e-04\n","Epoch 141/250\n","13/13 [==============================] - 1s 46ms/step - loss: 0.0165 - val_loss: 0.0503 - lr: 3.2768e-04\n","Epoch 142/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0163 - val_loss: 0.0512 - lr: 3.2768e-04\n","Epoch 143/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0152 - val_loss: 0.0509 - lr: 3.2768e-04\n","Epoch 144/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0161 - val_loss: 0.0504 - lr: 3.2768e-04\n","Epoch 145/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0156 - val_loss: 0.0502 - lr: 3.2768e-04\n","Epoch 146/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0151 - val_loss: 0.0504 - lr: 3.2768e-04\n","Epoch 147/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0164 - val_loss: 0.0503 - lr: 3.2768e-04\n","Epoch 148/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0156 - val_loss: 0.0505 - lr: 3.2768e-04\n","Epoch 149/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0165 - val_loss: 0.0509 - lr: 3.2768e-04\n","Epoch 150/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0158 - val_loss: 0.0508 - lr: 3.2768e-04\n","Epoch 151/250\n","13/13 [==============================] - 0s 39ms/step - loss: 0.0155 - val_loss: 0.0502 - lr: 2.6214e-04\n","Epoch 152/250\n","13/13 [==============================] - 1s 44ms/step - loss: 0.0151 - val_loss: 0.0499 - lr: 2.6214e-04\n","Epoch 153/250\n","13/13 [==============================] - 1s 45ms/step - loss: 0.0151 - val_loss: 0.0495 - lr: 2.6214e-04\n","Epoch 154/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0150 - val_loss: 0.0501 - lr: 2.6214e-04\n","Epoch 155/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0155 - val_loss: 0.0501 - lr: 2.6214e-04\n","Epoch 156/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0148 - val_loss: 0.0497 - lr: 2.6214e-04\n","Epoch 157/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0148 - val_loss: 0.0499 - lr: 2.6214e-04\n","Epoch 158/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0142 - val_loss: 0.0498 - lr: 2.6214e-04\n","Epoch 159/250\n","13/13 [==============================] - 1s 42ms/step - loss: 0.0142 - val_loss: 0.0495 - lr: 2.6214e-04\n","Epoch 160/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0155 - val_loss: 0.0501 - lr: 2.6214e-04\n","Epoch 161/250\n","13/13 [==============================] - 1s 49ms/step - loss: 0.0146 - val_loss: 0.0496 - lr: 2.6214e-04\n","Epoch 162/250\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0145 - val_loss: 0.0502 - lr: 2.6214e-04\n","Epoch 163/250\n","13/13 [==============================] - 1s 55ms/step - loss: 0.0139 - val_loss: 0.0497 - lr: 2.0972e-04\n","Epoch 164/250\n","13/13 [==============================] - 1s 51ms/step - loss: 0.0136 - val_loss: 0.0491 - lr: 2.0972e-04\n","Epoch 165/250\n","13/13 [==============================] - 1s 61ms/step - loss: 0.0132 - val_loss: 0.0488 - lr: 2.0972e-04\n","Epoch 166/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0139 - val_loss: 0.0491 - lr: 2.0972e-04\n","Epoch 167/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0139 - val_loss: 0.0494 - lr: 2.0972e-04\n","Epoch 168/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0132 - val_loss: 0.0503 - lr: 2.0972e-04\n","Epoch 169/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0142 - val_loss: 0.0499 - lr: 2.0972e-04\n","Epoch 170/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0144 - val_loss: 0.0493 - lr: 1.6777e-04\n","Epoch 171/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0130 - val_loss: 0.0495 - lr: 1.6777e-04\n","Epoch 172/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0137 - val_loss: 0.0490 - lr: 1.6777e-04\n","Epoch 173/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0130 - val_loss: 0.0494 - lr: 1.6777e-04\n","Epoch 174/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0136 - val_loss: 0.0491 - lr: 1.6777e-04\n","Epoch 175/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0133 - val_loss: 0.0497 - lr: 1.6777e-04\n","Epoch 176/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0128 - val_loss: 0.0498 - lr: 1.3422e-04\n","Epoch 177/250\n","13/13 [==============================] - 1s 38ms/step - loss: 0.0128 - val_loss: 0.0492 - lr: 1.3422e-04\n","Epoch 178/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0127 - val_loss: 0.0495 - lr: 1.3422e-04\n","Epoch 179/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0129 - val_loss: 0.0498 - lr: 1.3422e-04\n","Epoch 180/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0124 - val_loss: 0.0490 - lr: 1.3422e-04\n","Epoch 181/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0120 - val_loss: 0.0488 - lr: 1.3422e-04\n","Epoch 182/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0120 - val_loss: 0.0492 - lr: 1.3422e-04\n","Epoch 183/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0129 - val_loss: 0.0492 - lr: 1.3422e-04\n","Epoch 184/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0124 - val_loss: 0.0490 - lr: 1.3422e-04\n","Epoch 185/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0129 - val_loss: 0.0491 - lr: 1.3422e-04\n","Epoch 186/250\n","13/13 [==============================] - 1s 54ms/step - loss: 0.0119 - val_loss: 0.0492 - lr: 1.0737e-04\n","Epoch 187/250\n","13/13 [==============================] - 1s 59ms/step - loss: 0.0120 - val_loss: 0.0489 - lr: 1.0737e-04\n","Epoch 188/250\n","13/13 [==============================] - 1s 52ms/step - loss: 0.0123 - val_loss: 0.0488 - lr: 1.0737e-04\n","Epoch 189/250\n","13/13 [==============================] - 1s 57ms/step - loss: 0.0125 - val_loss: 0.0491 - lr: 1.0737e-04\n","Epoch 190/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0122 - val_loss: 0.0492 - lr: 1.0737e-04\n","Epoch 191/250\n","13/13 [==============================] - 0s 34ms/step - loss: 0.0118 - val_loss: 0.0493 - lr: 1.0000e-04\n","Epoch 192/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0120 - val_loss: 0.0495 - lr: 1.0000e-04\n","Epoch 193/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0122 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 194/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0116 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 195/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0123 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 196/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0116 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 197/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0120 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 198/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0121 - val_loss: 0.0493 - lr: 1.0000e-04\n","Epoch 199/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0119 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 200/250\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0122 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 201/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0120 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 202/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0123 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 203/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0120 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 204/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0117 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 205/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0121 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 206/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0123 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 207/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0119 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 208/250\n","13/13 [==============================] - 1s 43ms/step - loss: 0.0115 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 209/250\n","13/13 [==============================] - 1s 45ms/step - loss: 0.0123 - val_loss: 0.0484 - lr: 1.0000e-04\n","Epoch 210/250\n","13/13 [==============================] - 1s 62ms/step - loss: 0.0123 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 211/250\n","13/13 [==============================] - 1s 49ms/step - loss: 0.0132 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 212/250\n","13/13 [==============================] - 1s 63ms/step - loss: 0.0122 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 213/250\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0117 - val_loss: 0.0484 - lr: 1.0000e-04\n","Epoch 214/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0114 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 215/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0118 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 216/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0117 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 217/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0118 - val_loss: 0.0485 - lr: 1.0000e-04\n","Epoch 218/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0115 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 219/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0117 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 220/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0121 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 221/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0113 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 222/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0122 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 223/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0117 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 224/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0120 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 225/250\n","13/13 [==============================] - 0s 37ms/step - loss: 0.0111 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 226/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0112 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 227/250\n","13/13 [==============================] - 1s 42ms/step - loss: 0.0119 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 228/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0109 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 229/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0119 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 230/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0111 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 231/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0114 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 232/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0113 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 233/250\n","13/13 [==============================] - 1s 42ms/step - loss: 0.0114 - val_loss: 0.0485 - lr: 1.0000e-04\n","Epoch 234/250\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0112 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 235/250\n","13/13 [==============================] - 1s 59ms/step - loss: 0.0108 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 236/250\n","13/13 [==============================] - 1s 56ms/step - loss: 0.0116 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 237/250\n","13/13 [==============================] - 1s 53ms/step - loss: 0.0119 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 238/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0114 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 239/250\n","13/13 [==============================] - 1s 41ms/step - loss: 0.0113 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 240/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0112 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 241/250\n","13/13 [==============================] - 0s 36ms/step - loss: 0.0114 - val_loss: 0.0489 - lr: 1.0000e-04\n","Epoch 242/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0117 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 243/250\n","13/13 [==============================] - 1s 40ms/step - loss: 0.0111 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 244/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0113 - val_loss: 0.0488 - lr: 1.0000e-04\n","Epoch 245/250\n","13/13 [==============================] - 0s 35ms/step - loss: 0.0124 - val_loss: 0.0491 - lr: 1.0000e-04\n","Epoch 246/250\n","13/13 [==============================] - 1s 39ms/step - loss: 0.0110 - val_loss: 0.0487 - lr: 1.0000e-04\n","Epoch 247/250\n","13/13 [==============================] - 0s 38ms/step - loss: 0.0112 - val_loss: 0.0490 - lr: 1.0000e-04\n","Epoch 248/250\n","13/13 [==============================] - ETA: 0s - loss: 0.0113Restoring model weights from the end of the best epoch: 213.\n","13/13 [==============================] - 1s 44ms/step - loss: 0.0113 - val_loss: 0.0486 - lr: 1.0000e-04\n","Epoch 248: early stopping\n"]}]},{"cell_type":"code","source":["#@title Sample Prediction with Best Weights\n","\n","from tensorflow.keras.models import load_model\n","'''either load or use existing model'''\n","# modelcp = load_model('/content/best_checkpoint')\n","scroll = 32\n","predictions = model.predict(V[::898//scroll])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlQRRXY_x-Ei","executionInfo":{"status":"ok","timestamp":1703698000754,"user_tz":-210,"elapsed":3387,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}},"outputId":"6669239a-00e8-4621-d2fc-d6c60e37fb74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 2s 740ms/step\n"]}]},{"cell_type":"code","source":["for n in range(scroll):\n","  print(f'Predicted:{np.round(predictions[n],3)}\\nExpected: {np.round(Yv[n],3)} \\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvFbJPCBhar8","executionInfo":{"status":"ok","timestamp":1703698000755,"user_tz":-210,"elapsed":7,"user":{"displayName":"Parham Faraji","userId":"03849556059769072038"}},"outputId":"052475d5-6f30-4428-97eb-3c65bc87d559"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted:[0.379 0.348]\n","Expected: [0.333 0.365] \n","\n","Predicted:[0.495 0.218]\n","Expected: [0.5  0.24] \n","\n","Predicted:[0.634 0.177]\n","Expected: [0.167 0.144] \n","\n","Predicted:[0.46  0.351]\n","Expected: [0.5   0.269] \n","\n","Predicted:[0.423 0.197]\n","Expected: [0.5   0.202] \n","\n","Predicted:[0.676 0.341]\n","Expected: [0.333 0.308] \n","\n","Predicted:[0.333 0.228]\n","Expected: [0.333 0.337] \n","\n","Predicted:[0.392 0.342]\n","Expected: [0.5   0.317] \n","\n","Predicted:[0.36  0.327]\n","Expected: [0.167 0.327] \n","\n","Predicted:[0.495 0.278]\n","Expected: [0.833 0.337] \n","\n","Predicted:[0.341 0.321]\n","Expected: [0.5   0.231] \n","\n","Predicted:[0.56  0.314]\n","Expected: [0.667 0.385] \n","\n","Predicted:[0.466 0.303]\n","Expected: [0.333 0.346] \n","\n","Predicted:[0.539 0.316]\n","Expected: [0.5  0.24] \n","\n","Predicted:[0.565 0.31 ]\n","Expected: [0.667 0.365] \n","\n","Predicted:[0.505 0.387]\n","Expected: [0.333 0.087] \n","\n","Predicted:[0.404 0.252]\n","Expected: [0.667 0.115] \n","\n","Predicted:[0.598 0.382]\n","Expected: [0.833 0.375] \n","\n","Predicted:[0.491 0.431]\n","Expected: [0.333 0.442] \n","\n","Predicted:[0.502 0.307]\n","Expected: [0.667 0.423] \n","\n","Predicted:[0.492 0.246]\n","Expected: [0.5   0.298] \n","\n","Predicted:[0.67  0.378]\n","Expected: [0.167 0.192] \n","\n","Predicted:[0.386 0.154]\n","Expected: [0.5   0.288] \n","\n","Predicted:[0.501 0.251]\n","Expected: [0.667 0.365] \n","\n","Predicted:[0.617 0.286]\n","Expected: [0.333 0.385] \n","\n","Predicted:[0.664 0.232]\n","Expected: [0.667 0.298] \n","\n","Predicted:[0.538 0.184]\n","Expected: [0.667 0.413] \n","\n","Predicted:[0.388 0.273]\n","Expected: [0.5   0.288] \n","\n","Predicted:[0.666 0.288]\n","Expected: [0.5   0.221] \n","\n","Predicted:[0.628 0.22 ]\n","Expected: [0.5   0.337] \n","\n","Predicted:[0.438 0.298]\n","Expected: [0.667 0.346] \n","\n","Predicted:[0.227 0.321]\n","Expected: [0.667 0.356] \n","\n"]}]}]}